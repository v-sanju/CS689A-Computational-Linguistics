{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7832759,"sourceType":"datasetVersion","datasetId":4590742},{"sourceId":7834284,"sourceType":"datasetVersion","datasetId":4591899}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install transformers\n!pip3 install datasets\n!pip3 install sentencepiece\n!pip3 install seqeval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-13T14:01:04.738250Z","iopub.execute_input":"2024-03-13T14:01:04.738633Z","iopub.status.idle":"2024-03-13T14:02:11.927388Z","shell.execute_reply.started":"2024-03-13T14:01:04.738600Z","shell.execute_reply":"2024-03-13T14:02:11.925731Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=e3df4599c54e28363f562f5fd79aec648b875643744cd0791ffef34984d29adc\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Running the IndicNER Model\nLet's try annotating some Indian language sentences and get the named entities","metadata":{}},{"cell_type":"code","source":"# Import all the necessary classes and initialize the tokenizer and model.\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicNER\")\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"ai4bharat/IndicNER\")","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:02:22.140386Z","iopub.execute_input":"2024-03-13T14:02:22.140843Z","iopub.status.idle":"2024-03-13T14:02:36.955834Z","shell.execute_reply.started":"2024-03-13T14:02:22.140802Z","shell.execute_reply":"2024-03-13T14:02:36.954585Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/346 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"703a9ac4c2ac4783b9ad2288d37fb590"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37966630a99f4b38ae9dd36af6182afd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e76cf4aaf040401fbe72da896ebc6c32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f16471f5b2254d6a9bdb6269868c361c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c288359ee3e749e4be00fd821639d453"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/667M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7146499a2de24beb81bbe45bb4805214"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_predictions( sentence, tokenizer, model ):\n  # Let us first tokenize the sentence - split words into subwords\n  tok_sentence = tokenizer(sentence, return_tensors='pt') # argument return_tensors='pt' indicates return type is pytorch tensors\n\n  with torch.no_grad():\n    # we will send the tokenized sentence to the model to get predictions\n    logits = model(**tok_sentence).logits.argmax(-1) #once logits are found of tokenized sentence, argmax finds the maximum value along last dimension for each token in the sentence.\n\n    # We will map the maximum predicted class id with the class label\n    predicted_tokens_classes = [model.config.id2label[t.item()] for t in logits[0]]\n#     print(predicted_tokens_classes)\n    predicted_labels = []\n\n    previous_token_id = 0\n    # we need to assign the named entity label to the head word and not the following sub-words\n    word_ids = tok_sentence.word_ids()\n    for word_index in range(len(word_ids)):\n        if word_ids[word_index] == None:\n            previous_token_id = word_ids[word_index]\n        elif word_ids[word_index] == previous_token_id:\n            previous_token_id = word_ids[word_index]\n        else:\n            predicted_labels.append( predicted_tokens_classes[ word_index ] )\n            previous_token_id = word_ids[word_index]\n\n    return predicted_labels","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:02:36.959946Z","iopub.execute_input":"2024-03-13T14:02:36.960656Z","iopub.status.idle":"2024-03-13T14:02:36.971018Z","shell.execute_reply.started":"2024-03-13T14:02:36.960605Z","shell.execute_reply":"2024-03-13T14:02:36.969996Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# let us try with some example sentences here\nsentence = 'लगातार हमलावर हो रहे शिवपाल और राजभर को सपा की दो टूक, चिट्ठी जारी कर कहा- जहां जाना चाहें जा सकते हैं'\n\npredicted_labels = get_predictions(sentence=sentence,\n                                   tokenizer=tokenizer,\n                                   model=model\n                                   )\n\nfor index in range(len(sentence.split(' '))):\n  print( sentence.split(' ')[index] + '\\t' + predicted_labels[index] )","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:02:36.972667Z","iopub.execute_input":"2024-03-13T14:02:36.973425Z","iopub.status.idle":"2024-03-13T14:02:37.255733Z","shell.execute_reply.started":"2024-03-13T14:02:36.973384Z","shell.execute_reply":"2024-03-13T14:02:37.254417Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"लगातार\tO\nहमलावर\tO\nहो\tO\nरहे\tO\nशिवपाल\tB-PER\nऔर\tO\nराजभर\tB-PER\nको\tO\nसपा\tB-ORG\nकी\tO\nदो\tO\nटूक,\tO\nचिट्ठी\tO\nजारी\tO\nकर\tO\nकहा-\tO\nजहां\tO\nजाना\tO\nचाहें\tO\nजा\tO\nसकते\tO\nहैं\tO\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Naampadam Dataset for hindi Language\nThe Naampadam Dataset is a large dataset for Named Entity Recognition in 11 Indian languages. Naampadam means \"named entity\" in Sanskrit.\n\nHere I have used Hindi Dataset","metadata":{}},{"cell_type":"code","source":"# Let's download the Naampadam (Indic NER) dataset\nfrom datasets import ClassLabel, load_dataset, load_metric, DownloadMode\n\nlang='hi'\n\nraw_datasets = load_dataset('ai4bharat/naamapadam', lang)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:04:28.119167Z","iopub.execute_input":"2024-03-13T14:04:28.119634Z","iopub.status.idle":"2024-03-13T14:08:52.172754Z","shell.execute_reply.started":"2024-03-13T14:04:28.119602Z","shell.execute_reply":"2024-03-13T14:08:52.171897Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.86k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e86d895d19fd47438a911d153f88c1d1"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset naamapadam_pr/hi to /root/.cache/huggingface/datasets/ai4bharat___naamapadam_pr/hi/1.0.0/99b5ec77eabfaa3fbff510d8cf70d7c34519486cb7dbee99ede19474ddff9b20...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/82.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df8cf3a0432145b4ab331ba5653bad7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset naamapadam_pr downloaded and prepared to /root/.cache/huggingface/datasets/ai4bharat___naamapadam_pr/hi/1.0.0/99b5ec77eabfaa3fbff510d8cf70d7c34519486cb7dbee99ede19474ddff9b20. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9841ba5fc3e94832a31eb0f51a8e5bcd"}},"metadata":{}}]},{"cell_type":"code","source":"# let's now print how the Dataset looks like\nraw_datasets","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:08:52.174569Z","iopub.execute_input":"2024-03-13T14:08:52.175605Z","iopub.status.idle":"2024-03-13T14:08:52.182963Z","shell.execute_reply.started":"2024-03-13T14:08:52.175567Z","shell.execute_reply":"2024-03-13T14:08:52.181612Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 985787\n    })\n    test: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 867\n    })\n    validation: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 13460\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"raw_datasets.column_names","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:08:52.184870Z","iopub.execute_input":"2024-03-13T14:08:52.185250Z","iopub.status.idle":"2024-03-13T14:08:52.200997Z","shell.execute_reply.started":"2024-03-13T14:08:52.185217Z","shell.execute_reply":"2024-03-13T14:08:52.199733Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'train': ['tokens', 'ner_tags'],\n 'test': ['tokens', 'ner_tags'],\n 'validation': ['tokens', 'ner_tags']}"},"metadata":{}}]},{"cell_type":"code","source":"# let's print an instance of dataset\nidx=985786 # last statement in training set.\nrec=raw_datasets['train'][idx]\nfor w, t in zip(rec['tokens'],rec['ner_tags']): # zip() function iterates over the given two lists simultaneously\n  print('{}\\t{}'.format(w,t))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:08:52.203920Z","iopub.execute_input":"2024-03-13T14:08:52.204369Z","iopub.status.idle":"2024-03-13T14:08:52.220171Z","shell.execute_reply.started":"2024-03-13T14:08:52.204336Z","shell.execute_reply":"2024-03-13T14:08:52.219132Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"उनके\t0\n27\t0\nसाल\t0\nके\t0\nबेटे\t0\nजीवा\t1\nको\t0\nभी\t0\nदिल\t0\nकी\t0\nबीमारी\t0\nथी\t0\n।\t0\n","output_type":"stream"}]},{"cell_type":"code","source":"column_names = raw_datasets[\"train\"].column_names\nprint(column_names) # names of the columns in dataset\n\nfeatures = raw_datasets[\"train\"].features\nprint(features)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:08:52.221129Z","iopub.execute_input":"2024-03-13T14:08:52.221450Z","iopub.status.idle":"2024-03-13T14:08:52.234536Z","shell.execute_reply.started":"2024-03-13T14:08:52.221421Z","shell.execute_reply":"2024-03-13T14:08:52.233474Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"['tokens', 'ner_tags']\n{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)}\n","output_type":"stream"}]},{"cell_type":"code","source":"text_column_name = \"tokens\"\nlabel_column_name = \"ner_tags\"","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:08:52.235935Z","iopub.execute_input":"2024-03-13T14:08:52.236341Z","iopub.status.idle":"2024-03-13T14:08:52.249111Z","shell.execute_reply.started":"2024-03-13T14:08:52.236306Z","shell.execute_reply":"2024-03-13T14:08:52.248089Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# this shows the ClassLabel objects mapping to numbers\n\nlabel_list = features[label_column_name].feature.names # gets the lables list\n\nlabel_to_id = {label_list[i]: features[label_column_name].feature.str2int( label_list[i] ) for i in range(len(label_list))} # mapping of lables to id.\n\nprint(label_to_id)\n\nnum_labels = len(label_list)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:08:52.250340Z","iopub.execute_input":"2024-03-13T14:08:52.250733Z","iopub.status.idle":"2024-03-13T14:08:52.265318Z","shell.execute_reply.started":"2024-03-13T14:08:52.250697Z","shell.execute_reply":"2024-03-13T14:08:52.264005Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Pre-Processing before training\n### Tokenize the dataset and align tokens with their corresponding NER tags","metadata":{}},{"cell_type":"code","source":"# Tokenize all texts and align the labels with them.\npadding = \"max_length\"\ndef tokenize_and_align_labels(examples, tokenizer): # added tokenizer as an argument\n    tokenized_inputs = tokenizer(\n        examples[text_column_name],\n        padding=padding,\n        truncation=True,\n        max_length=512,\n        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n        is_split_into_words=True,\n    )\n    labels = []\n    for i, label in enumerate(examples[label_column_name]):\n        # print('=====')\n        # print('{} {}'.format(i,label)) #ak\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n            # ignored in the loss function.\n            if word_idx is None:\n                label_ids.append(-100)\n            # We set the label for the first token of each word.\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n            # the label_all_tokens flag.\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:22:54.233321Z","iopub.execute_input":"2024-03-13T09:22:54.234097Z","iopub.status.idle":"2024-03-13T09:22:54.241882Z","shell.execute_reply.started":"2024-03-13T09:22:54.234067Z","shell.execute_reply":"2024-03-13T09:22:54.240921Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Metrics\nmetric = load_metric(\"seqeval\")\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    # Unpack nested dictionaries\n    final_results = {}\n    for key, value in results.items():\n        if isinstance(value, dict):\n            for n, v in value.items():\n                final_results[f\"{key}_{n}\"] = v\n        else:\n            final_results[key] = value\n    return final_results","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:22:54.915392Z","iopub.execute_input":"2024-03-13T09:22:54.915759Z","iopub.status.idle":"2024-03-13T09:22:55.993074Z","shell.execute_reply.started":"2024-03-13T09:22:54.915725Z","shell.execute_reply":"2024-03-13T09:22:55.992269Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25d20581e8b64e20b9cf5ecc8af6c30c"}},"metadata":{}}]},{"cell_type":"markdown","source":"taking 20000 instances of training data","metadata":{}},{"cell_type":"code","source":"rtrain_dataset = raw_datasets['train'].select(range(20000))","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:22:57.572936Z","iopub.execute_input":"2024-03-13T09:22:57.573600Z","iopub.status.idle":"2024-03-13T09:22:57.585429Z","shell.execute_reply.started":"2024-03-13T09:22:57.573567Z","shell.execute_reply":"2024-03-13T09:22:57.584374Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"!pip install -U accelerate # for hardware optimization\n!pip install transformers[torch] # ensuring all the dependencies for running PyTorch","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:22:58.887985Z","iopub.execute_input":"2024-03-13T09:22:58.888838Z","iopub.status.idle":"2024-03-13T09:23:24.289366Z","shell.execute_reply.started":"2024-03-13T09:22:58.888805Z","shell.execute_reply":"2024-03-13T09:23:24.288214Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nCollecting accelerate\n  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.27.2\n    Uninstalling accelerate-0.27.2:\n      Successfully uninstalled accelerate-0.27.2\nSuccessfully installed accelerate-0.28.0\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.38.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.28.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fine Tunning indicBERT for NER task using Hindi Nampadam dataset\nWe have already seen how to get predictions from fine-tuned NER model. We will now use the pre-trained IndicBERT model and fine-tune it for NER task.\n\nLet us download a pre-trained model and fine-tune it for the task of NER. We will have to use the AutoModelForTokenClassification class to fine-tune the model","metadata":{}},{"cell_type":"markdown","source":"### Load Pre-trained Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback, IntervalStrategy\nimport numpy as np\n\nconfig = AutoConfig.from_pretrained('ai4bharat/indic-bert', num_labels=num_labels, finetuning_task='ner')\nindic_tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\nindic_model = AutoModelForTokenClassification.from_pretrained('ai4bharat/indic-bert', num_labels=num_labels )","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:23:24.291930Z","iopub.execute_input":"2024-03-13T09:23:24.292732Z","iopub.status.idle":"2024-03-13T09:23:39.008875Z","shell.execute_reply.started":"2024-03-13T09:23:24.292678Z","shell.execute_reply":"2024-03-13T09:23:39.008101Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"2024-03-13 09:23:26.118590: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-13 09:23:26.118689: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-13 09:23:26.239924: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f57c537e283b46fda916a4f30ce5acc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9267d67b2d504d4eb28301a78745b8a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9bb03fa94934eda9a5d65785804fc45"}},"metadata":{}},{"name":"stderr","text":"Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# aligning test dataset in token:ner_tag pair\nindic_train_dataset = rtrain_dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=4,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on train dataset\",\n    fn_kwargs={\"tokenizer\": indic_tokenizer}\n) ","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:23:56.984417Z","iopub.execute_input":"2024-03-13T09:23:56.984839Z","iopub.status.idle":"2024-03-13T09:24:04.950086Z","shell.execute_reply.started":"2024-03-13T09:23:56.984805Z","shell.execute_reply":"2024-03-13T09:24:04.949098Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"     ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #0:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51cf339774aa498d8e73fd81f324b65e"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #1:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6414fef943b485d8a7ce5c3a01506a7"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #2:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"635a03b96b144d79b032561df7784639"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on train dataset #3:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63d53440af3f413f91d37def2961daba"}},"metadata":{}}]},{"cell_type":"code","source":"# aligning validation dataset in token:ner_tag pair\neval_dataset = raw_datasets[\"validation\"]\nindic_eval_dataset = eval_dataset.map(\n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=4,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on Validation dataset\",\n    fn_kwargs={\"tokenizer\": indic_tokenizer}\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:24:04.951905Z","iopub.execute_input":"2024-03-13T09:24:04.952175Z","iopub.status.idle":"2024-03-13T09:24:11.310481Z","shell.execute_reply.started":"2024-03-13T09:24:04.952150Z","shell.execute_reply":"2024-03-13T09:24:11.309342Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"     ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #0:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4a867314ba94066b41f73b244839dd6"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #1:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1ab229cbe004e0daaf940f6b2ad7e23"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #2:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abe02169ba384989bcca736c668c4d14"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on Validation dataset #3:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"059bbe88221f4faba814bcc202b41d2c"}},"metadata":{}}]},{"cell_type":"code","source":"# setting training arguments\nbatch_size=8\nargs=TrainingArguments(\n    output_dir='output_dir',\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-6)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:24:24.179577Z","iopub.execute_input":"2024-03-13T09:24:24.180558Z","iopub.status.idle":"2024-03-13T09:24:24.215331Z","shell.execute_reply.started":"2024-03-13T09:24:24.180511Z","shell.execute_reply":"2024-03-13T09:24:24.214347Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"indic_data_collator = DataCollatorForTokenClassification(indic_tokenizer) # will use in training","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:24:27.056953Z","iopub.execute_input":"2024-03-13T09:24:27.057643Z","iopub.status.idle":"2024-03-13T09:24:27.062021Z","shell.execute_reply.started":"2024-03-13T09:24:27.057611Z","shell.execute_reply":"2024-03-13T09:24:27.061055Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Initialize our Trainer\n\nindic_trainer = Trainer(\n    model = indic_model,\n    train_dataset=indic_train_dataset,\n    eval_dataset=indic_eval_dataset,\n    tokenizer=indic_tokenizer,\n    data_collator=indic_data_collator,\n    compute_metrics=compute_metrics,\n    # callbacks=[early_stopping_callback],\n    args=args,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:24:28.546582Z","iopub.execute_input":"2024-03-13T09:24:28.547505Z","iopub.status.idle":"2024-03-13T09:24:29.605089Z","shell.execute_reply.started":"2024-03-13T09:24:28.547469Z","shell.execute_reply":"2024-03-13T09:24:29.604327Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"indic_trainer.args # just to see training arguments","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:24:33.283832Z","iopub.execute_input":"2024-03-13T09:24:33.284649Z","iopub.status.idle":"2024-03-13T09:24:33.291869Z","shell.execute_reply.started":"2024-03-13T09:24:33.284618Z","shell.execute_reply":"2024-03-13T09:24:33.290932Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=None,\nevaluation_strategy=epoch,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-06,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output_dir/runs/Mar13_09-24-24_37947a10d924,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3,\noptim=adamw_torch,\noptim_args=None,\noutput_dir=output_dir,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=8,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nresume_from_checkpoint=None,\nrun_name=output_dir,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=steps,\nsave_total_limit=None,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=0,\nweight_decay=0.0,\n)"},"metadata":{}}]},{"cell_type":"code","source":"# training the model\nindic_train_result = indic_trainer.train()\nmetrics = indic_train_result.metrics","metadata":{"execution":{"iopub.status.busy":"2024-03-13T09:24:34.532445Z","iopub.execute_input":"2024-03-13T09:24:34.532818Z","iopub.status.idle":"2024-03-13T10:31:23.228231Z","shell.execute_reply.started":"2024-03-13T09:24:34.532788Z","shell.execute_reply":"2024-03-13T10:31:23.227011Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240313_092503-6rr9aysc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sanju240/huggingface/runs/6rr9aysc' target=\"_blank\">cool-pine-8</a></strong> to <a href='https://wandb.ai/sanju240/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sanju240/huggingface' target=\"_blank\">https://wandb.ai/sanju240/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sanju240/huggingface/runs/6rr9aysc' target=\"_blank\">https://wandb.ai/sanju240/huggingface/runs/6rr9aysc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7500/7500 1:05:47, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Loc Precision</th>\n      <th>Loc Recall</th>\n      <th>Loc F1</th>\n      <th>Loc Number</th>\n      <th>Org Precision</th>\n      <th>Org Recall</th>\n      <th>Org F1</th>\n      <th>Org Number</th>\n      <th>Per Precision</th>\n      <th>Per Recall</th>\n      <th>Per F1</th>\n      <th>Per Number</th>\n      <th>Overall Precision</th>\n      <th>Overall Recall</th>\n      <th>Overall F1</th>\n      <th>Overall Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.504900</td>\n      <td>0.495392</td>\n      <td>0.505526</td>\n      <td>0.277685</td>\n      <td>0.358466</td>\n      <td>10213</td>\n      <td>0.247794</td>\n      <td>0.068874</td>\n      <td>0.107788</td>\n      <td>9786</td>\n      <td>0.486060</td>\n      <td>0.326646</td>\n      <td>0.390719</td>\n      <td>10568</td>\n      <td>0.451140</td>\n      <td>0.227762</td>\n      <td>0.302702</td>\n      <td>0.858342</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.426500</td>\n      <td>0.434264</td>\n      <td>0.526862</td>\n      <td>0.417703</td>\n      <td>0.465975</td>\n      <td>10213</td>\n      <td>0.316867</td>\n      <td>0.248416</td>\n      <td>0.278497</td>\n      <td>9786</td>\n      <td>0.583883</td>\n      <td>0.443603</td>\n      <td>0.504167</td>\n      <td>10568</td>\n      <td>0.478402</td>\n      <td>0.372460</td>\n      <td>0.418836</td>\n      <td>0.874081</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.401800</td>\n      <td>0.420833</td>\n      <td>0.514901</td>\n      <td>0.456771</td>\n      <td>0.484097</td>\n      <td>10213</td>\n      <td>0.355334</td>\n      <td>0.255263</td>\n      <td>0.297098</td>\n      <td>9786</td>\n      <td>0.576708</td>\n      <td>0.478425</td>\n      <td>0.522989</td>\n      <td>10568</td>\n      <td>0.491572</td>\n      <td>0.399745</td>\n      <td>0.440928</td>\n      <td>0.877084</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\n# Assuming indic_trainer is your Trainer object and you've trained the model\n# indic_train_result = indic_trainer.train()\ntrained_model = indic_trainer.model\n\n# Save the trained model\ntrained_model.save_pretrained(\"/kaggle/working/indic_trainer\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T10:34:08.542142Z","iopub.execute_input":"2024-03-13T10:34:08.542563Z","iopub.status.idle":"2024-03-13T10:34:08.859034Z","shell.execute_reply.started":"2024-03-13T10:34:08.542533Z","shell.execute_reply":"2024-03-13T10:34:08.857970Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"!zip -r BertTuned.zip /kaggle/working/indic_trainer","metadata":{"execution":{"iopub.status.busy":"2024-03-13T10:34:22.737116Z","iopub.execute_input":"2024-03-13T10:34:22.737999Z","iopub.status.idle":"2024-03-13T10:34:30.558829Z","shell.execute_reply.started":"2024-03-13T10:34:22.737963Z","shell.execute_reply":"2024-03-13T10:34:30.557805Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/indic_trainer/ (stored 0%)\n  adding: kaggle/working/indic_trainer/model.safetensors (deflated 7%)\n  adding: kaggle/working/indic_trainer/config.json (deflated 56%)\n","output_type":"stream"}]},{"cell_type":"code","source":"metrics = indic_trainer.evaluate()\nindic_trainer.log_metrics(\"eval\", metrics)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T10:35:08.671871Z","iopub.execute_input":"2024-03-13T10:35:08.672588Z","iopub.status.idle":"2024-03-13T10:39:41.111606Z","shell.execute_reply.started":"2024-03-13T10:35:08.672555Z","shell.execute_reply":"2024-03-13T10:39:41.110335Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1683' max='1683' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1683/1683 04:16]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"***** eval metrics *****\n  epoch                   =        3.0\n  eval_LOC_f1             =     0.4841\n  eval_LOC_number         =      10213\n  eval_LOC_precision      =     0.5149\n  eval_LOC_recall         =     0.4568\n  eval_ORG_f1             =     0.2971\n  eval_ORG_number         =       9786\n  eval_ORG_precision      =     0.3553\n  eval_ORG_recall         =     0.2553\n  eval_PER_f1             =      0.523\n  eval_PER_number         =      10568\n  eval_PER_precision      =     0.5767\n  eval_PER_recall         =     0.4784\n  eval_loss               =     0.4208\n  eval_overall_accuracy   =     0.8771\n  eval_overall_f1         =     0.4409\n  eval_overall_precision  =     0.4916\n  eval_overall_recall     =     0.3997\n  eval_runtime            = 0:04:32.39\n  eval_samples_per_second =     49.414\n  eval_steps_per_second   =      6.179\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Evaluate the Trained Model¶\nLet us now evaluate the trained model on the test sets of all languages\n\nWe need to first tokenize the test sets","metadata":{}},{"cell_type":"code","source":"indic_tokenized_test_set = raw_datasets['test'].map(\n    tokenize_and_align_labels,\n    batched=True,\n    num_proc=6,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on test dataset\",\n    fn_kwargs={\"tokenizer\": indic_tokenizer}\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:29:52.062258Z","iopub.execute_input":"2024-03-12T20:29:52.062989Z","iopub.status.idle":"2024-03-12T20:29:55.876195Z","shell.execute_reply.started":"2024-03-12T20:29:52.062950Z","shell.execute_reply":"2024-03-12T20:29:55.874681Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"       ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c2699b445e344a8b2704cd4653f14cc"}},"metadata":{}},{"name":"stdout","text":"  ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ac77cc719c44a55a597def397f655e4"}},"metadata":{}},{"name":"stdout","text":"  ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9580f1cdf02f4418888859b18e377c4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c61a3d304dc4dcdbbe86807f9052d4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #4:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f661f9daaf64a76a9f8dbd8d0410f40"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on test dataset #5:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c28906bbc7b4cf28d22faf6bc99d625"}},"metadata":{}}]},{"cell_type":"markdown","source":"Run prediction on test set of each of the language separately and extract overall Precison, Recall and F-Score separately","metadata":{}},{"cell_type":"code","source":"indic_Evaluation_metric = {}\npredictions, labels, metrics = indic_trainer.predict(indic_tokenized_test_set)\nfor key in metrics:\n    if 'overall_precision' in key:\n      indic_Evaluation_metric['Precision'] = metrics[key]\n    elif 'overall_recall' in key:\n      indic_Evaluation_metric['Recall'] = metrics[key]\n    elif 'overall_f1' in key:\n      indic_Evaluation_metric['F1'] = metrics[key]\nindic_Evaluation_metric","metadata":{"execution":{"iopub.status.busy":"2024-03-12T20:29:55.881577Z","iopub.execute_input":"2024-03-12T20:29:55.881989Z","iopub.status.idle":"2024-03-12T20:30:12.981396Z","shell.execute_reply.started":"2024-03-12T20:29:55.881946Z","shell.execute_reply":"2024-03-12T20:30:12.980473Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'Precision': 0.6545648691938067,\n 'Recall': 0.6355624675997926,\n 'F1': 0.6449237243556023}"},"metadata":{}}]},{"cell_type":"markdown","source":"### Pre_processing for Q4","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/for-q4-indic-bert/q1 part1.txt') as f:\n    part1 = f.read()\nwith open('/kaggle/input/for-q4-indic-bert/q1 part 2.txt') as f:\n    part2 = f.read()\nwith open('/kaggle/input/for-q4-indic-bert/q1 part 3.txt') as f:\n    part3 = f.read()\nsentence = [part1,part2,part3]","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:09:15.439164Z","iopub.execute_input":"2024-03-13T14:09:15.439591Z","iopub.status.idle":"2024-03-13T14:09:15.456508Z","shell.execute_reply.started":"2024-03-13T14:09:15.439559Z","shell.execute_reply":"2024-03-13T14:09:15.455147Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"sentence","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:09:18.211226Z","iopub.execute_input":"2024-03-13T14:09:18.211748Z","iopub.status.idle":"2024-03-13T14:09:18.220838Z","shell.execute_reply.started":"2024-03-13T14:09:18.211690Z","shell.execute_reply":"2024-03-13T14:09:18.219026Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['इस बार कांग्रेस ने जो घोषणा पत्र जारी किया है उसमें आपके 6000 के सामने कांग्रेस के 72000 भारी पड़ेंगा? मोदी सरकार के पहले कार्यकाल में भी तीन तलाक को लेकर बिल लाया गया था, हालांकि तब यह राज्यसभा में पास नहीं हो पाया था. चुल्हे की संरचना सामान्यतया ब्युटेन पर चलने वाले चुल्हे के समान ही होती है परंतु इनके (बायोगैस चुल्हे) बर्नर में वायु छिद्र का आकार बड़ा होता है। उसने तुरंत बैंक में जाकर पता किया तो उसके खाते से किसी ने एटीएम के माध्यम से तीस हजार रुपये निकाल लिए थे। इससे पहले भी कई अफेयर हुए हैं जिनमें कुछ तो सफल रिलेशनशिप में तब्दील हो गए वहीं कुछ अफेयर सिर्फ अफसाना बनकर रह गए।  संविधान का ८० प्रतिशत कार्य पुरा हो चुका है । उनके पिता का कोई पता नहीं चल पाया. साथ ही शादी का सामान यानी गहनों और कपड़ों का भी कुछ पता नहीं चल पाया है. नीति आयोग के CEO अमिताभ कांत ने कहा कि देश में पिछले तीन साल में प्रति ग्राहक मोबाइल डाटा कंज्मप्शन में 142 फीसदी की ग्रोथ दर्ज की गई है।',\n 'इनेलो हर वर्ग के लोगों की सुरक्षा करने में सक्षम : सतबीर कादियान मीडिया रिपोर्टों के मुताबिक एक चर्च के बाहर सुरक्षाकर्मी एक महिला और उसके दो बच्चों को राेक कर उनसे पूछताछ कर रही थी। गौरतलब है कि शेयर बाजारों में मंगलवार को लगातार दूसरे दिन गिरावट आयी और सेंसेक्स 642 अंक टूटकर 36,481.09 अंक पर बंद हुआ। जीआरडी इंटरनेशनल कॉलेज में मनाया तीज का उत्सव | जीआरडी इंटरनेशनल कॉलेज में मनाया तीज का उत्सव - Dainik Bhaskar एक बार जापान के सिन्धी सेठ आसूमल द्वारा जापान में तकनीकी शिक्षा पाने वाले भारतीय छात्रों को दी […] उद्घाटन करने के बाद सभा को संबोधित करते हुए सीएम त्रिवेंद्र ने कहा कि क्षेत्र में प्रस्तावित पंचेश्वर बांध देश के लिए महत्वपूर्ण है, लिहाज़ डूब क्षेत्र की लोगों की जरूरतें पूरा करना राज्य सरकार कर्तव्य है। -12 बजे से 3 बजे तक चेतक ब्रिज से गोविंदपुरा की तरफ आने वाले वाहनों पर प्रतिबंध रहेगा| फारूक सुबह करीब 11:30 बजे सेक्टर-18 में ईडी कार्यालय पहुंचे और शाम 5:30 बजे बाहर आये।',\n 'के बारामुला में हुए एक सड़क हादसे में शेरपुर गांव के एक फौजी की मौत हो गई है। इस बीच , राष्ट्रपति भण्डारी ने कहा हमारें भारत भ्रमण से दों देशों के संबन्धों को मजबूती मिलेंगी । टेक्सास में निजी यात्री विमान दुर्घटनाग्रस्त, दस लोगों की मौत हाल ही में 18 साल की पूनम राणा ने भी नेहरु पर्वतारोहण संस्थान उत्तरकाशी से प्रशिक्षण लेकर एवरेस्ट फतह किया है. 14 मैचों में छह जीत के बाद अंक तालिका में वे पांचवें स्थान पर थी। लुटने के डर से व्यापारी ने दुकान की जगह थाने से बंटवाई यूरिया मध्यप्रदेश / अयोध्या पर फैसले की संभावना; पुलिसकर्मियों की छुट्टी रद्द, इंदौर टेस्ट के दौरान अतिरिक्त बल तैनात होगा प्राप्\\u200dत जानकारी के अनुसार यह हादसा रायबरेली के निकट हरचन्दपुर के बाबापुर के पास सुबह करीब छह बजे हुआ। इन लोगों ने बताया कि गांव में भूमिगत जल पीने योग्य नहीं है।\\n']"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback, IntervalStrategy\nimport numpy as np\n\nconfig = AutoConfig.from_pretrained('/kaggle/input/indic-bert-fine-tuned/kaggle/working/indic_trainer', num_labels=num_labels, finetuning_task='ner')\nindic_tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\nindic_model = AutoModelForTokenClassification.from_pretrained('/kaggle/input/indic-bert-fine-tuned/kaggle/working/indic_trainer')","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:10:20.314232Z","iopub.execute_input":"2024-03-13T14:10:20.314698Z","iopub.status.idle":"2024-03-13T14:10:41.639167Z","shell.execute_reply.started":"2024-03-13T14:10:20.314663Z","shell.execute_reply":"2024-03-13T14:10:41.637515Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"2024-03-13 14:10:23.657666: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-13 14:10:23.657820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-13 14:10:23.880912: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d4713be73c74e3f83aaae73f8ae6a46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65d8c0ec53724d3bba3fabd4e9e3ced3"}},"metadata":{}}]},{"cell_type":"code","source":"predicted_labels = []\nfor part in sentence:\n    labels = get_predictions(sentence=part,\n                                   tokenizer=indic_tokenizer,\n                                   model=indic_model\n                                   )\n    for tag in labels:\n        predicted_labels.append(tag)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:10:46.309437Z","iopub.execute_input":"2024-03-13T14:10:46.310222Z","iopub.status.idle":"2024-03-13T14:10:47.903005Z","shell.execute_reply.started":"2024-03-13T14:10:46.310152Z","shell.execute_reply":"2024-03-13T14:10:47.901846Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"label_mapping = {'0':'O', '1':'B-PER', '2':'I-PER', '3':'B-ORG', '4':'I-ORG', '5':'B-LOC', '6':'I-LOC'}","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:15:07.928373Z","iopub.execute_input":"2024-03-13T14:15:07.928791Z","iopub.status.idle":"2024-03-13T14:15:07.934789Z","shell.execute_reply.started":"2024-03-13T14:15:07.928761Z","shell.execute_reply":"2024-03-13T14:15:07.933665Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"pred_labels = []\nfor tag in predicted_labels:\n    pred_labels.append(label_mapping[tag[-1]])","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:16:29.913826Z","iopub.execute_input":"2024-03-13T14:16:29.914277Z","iopub.status.idle":"2024-03-13T14:16:29.920307Z","shell.execute_reply.started":"2024-03-13T14:16:29.914243Z","shell.execute_reply":"2024-03-13T14:16:29.918985Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# File path where you want to save the object\nfile_path = '/kaggle/working/indic_tags.pkl'\n\n# Open the file in binary write mode\nwith open(file_path, 'wb') as file:\n    # Serialize the object and write it to the file\n    pickle.dump(pred_labels, file)\n\nprint(\"Object saved to:\", file_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T14:17:22.010560Z","iopub.execute_input":"2024-03-13T14:17:22.010960Z","iopub.status.idle":"2024-03-13T14:17:22.018744Z","shell.execute_reply.started":"2024-03-13T14:17:22.010931Z","shell.execute_reply":"2024-03-13T14:17:22.017332Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Object saved to: /kaggle/working/indic_tags.pkl\n","output_type":"stream"}]}]}