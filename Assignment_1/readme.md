Sanjeev Kumar
CS689 Assignment 1

dataset used: hi_100_new

Changes required for running python notebook: change necessary paths of corpus before running the notebook.

Question 1:

	Fisrt created different lists and dictionaries for lookup while correcting unicode as disscussed in class.

	Functions used:
	1)unicode_corrected(word) -> this function takes a string word argument and returns a list of corrected unicodes. One sample is given in notebook for showing the working of function.

Question 2:

	First converted whole corpus in list of tokens.

	Functions Used:
	1) top20(list1) -> takes a list in input and prints top 20 elements based on decreasing frequencies.
	2) character_count(tokens) -> takes list of tokens and processes each token from that list calling unicode_corrected(word) function and returns a list of unigram charaters.
	3) character_bigrams(tokens) -> takes list of tokens and returns list of bi-gram characters.
	4) gen_syllable(word, slist) -> takes word and generate syllables in that word and updates those syllables in slist.
	5) syllable_unigrams(tokens) -> takes list of tokens and returns list of uni-gram syllables.
	6) syllable_bigrams(tokens) -> return a list of bi-gram syllables given a list of tokens 

Question 3:
	All 25 questions with their answers.

Question 4:

	Some Pre-Processing:
		Trained Unigram , BPE -> 1k, 2k models.
		
		Functions used:
		1) token_bigrams(tokens) -> returns list of bi-gram tokens given a list of tokens.
		2) uni_gram_frequencies(tokens) -> for printing uni-gram frequencies of tokens, characters and syllables of different tokenizer models given list of tokens of produced by that model.
		3) bi_gram_frequencies(tokens) -> for printing bi-gram frequencies of tokens, characters and syllables of different tokenizer models given list of tokens of produced by that model.
		4) algorithm_model(model,corpus) -> given tokenizer model and corpus, returns list of tokens for that model.
		
	Models Used: 
		1) Unigram algorithm
		2) BPE Algorithm Vocab size = 1k
		3) BPE Algorithm Vocab Size = 2k
		4) mBERT algorithm max_length = 1k
		5) mBERT algorithm max_length = 2k
		6) IndicBERT algorithm max_length = 1k
		7) IndicBERT algorithm max_length = 2k
		8) White-Space tokenizer
		For each of the models above tokens on full corpus and tokens on 25 questions given in q3 generated (for using in question 5).
		After generating tokens for each of the above models:
			Uni-gram frequencies of tokens, charcters and syllables printed for top 20 items.
			Bi-grma frequencies of tokens, charcters and syllables printed for top 20 items.

Question 5:

	Lists of tokens generated by different models for 25 sentences of question 3:
		1) tokens_uni_1k_25
		2) tokens_bpe_1k_25
		3) tokens_bpe_2k_25
		4) tokens_mbert_1k_25
		5) tokens_mbert_2k_25
		6) cleaned_indicBert_list_1k_uni_25
		7) cleaned_indicBert_list_2k_uni_25
		8) white_space_tokens_25
	Functions Used:
		1) clean_tokens(tokens) -> cleans some of the above token lists.
		2) calculate_metrics(predicted_tokens, true_tokens) -> returns tuple of (precision, recall and f-score) as percentage given predicted_tokens and true_tokens.
	Once precision, recall and f-score as percentage calculated using above functions for all the models mentioned in question 4, we printed them out for all the models -> Precision -> Recall -> F-Score